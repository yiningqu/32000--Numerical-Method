{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uy1pUOMx4GBP"
   },
   "source": [
    "Two of the functions have one or more places where you are to \"FILL THIS IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5muQAh24GBQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVrtWCVN4GBR"
   },
   "outputs": [],
   "source": [
    "class GBM:\n",
    "\n",
    "    def __init__(self,S0,r,sigma):\n",
    "        self.S0 = 1\n",
    "        self.r = 0.03\n",
    "        self.sigma = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsUtWRWg4GBR"
   },
   "outputs": [],
   "source": [
    "hw7dynamics = GBM(S0=1,r=0.03,sigma=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKfu_mkO4GBR"
   },
   "outputs": [],
   "source": [
    "class Put:\n",
    "\n",
    "    def __init__(self,K,T):\n",
    "        self.K = K\n",
    "        self.T = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lW_W1mKf4GBR"
   },
   "outputs": [],
   "source": [
    "hw7contract = Put(K=1.1,T=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94XU4gKI4GBR"
   },
   "outputs": [],
   "source": [
    "class MCengine:\n",
    "\n",
    "    def __init__(self,M,N,seed,algorithm):\n",
    "\n",
    "        self.M = M # Number of paths\n",
    "        self.N = N     # Number of time periods\n",
    "        self.rng = np.random.default_rng(seed=seed) # Seeding the random number generator with a specified number helps make the calculations reproducible\n",
    "\n",
    "        self.algorithm = algorithm\n",
    "        #'value' for Value-based approach (Longstaff-Schwartz) -- problem 1a\n",
    "        #'policy' for Policy optimization -- problem 1b\n",
    "\n",
    "    def price_americanPut_GBM(self,contract,dynamics):\n",
    "\n",
    "        r=dynamics.r\n",
    "        sigma=dynamics.sigma\n",
    "        S0=dynamics.S0\n",
    "\n",
    "        K=contract.K\n",
    "        T=contract.T\n",
    "\n",
    "        N=self.N\n",
    "        M=self.M\n",
    "        dt=T/N\n",
    "\n",
    "        Z = self.rng.normal(size=(M,N))\n",
    "\n",
    "        paths = S0*np.exp((r-sigma**2/2)*dt*np.tile(np.arange(1,N+1),(M,1))+sigma*np.sqrt(dt)*np.cumsum(Z,axis=1))\n",
    "\n",
    "        payoffDiscounted = np.maximum(0,K-paths[:,-1])\n",
    "        #This is the payoff (cashflow) along each path,\n",
    "        #discounted to time nn (for nn=N,N-1,...)\n",
    "        #It corresponds to the far right-hand column in each page of the\n",
    "        #Excel worksheet\n",
    "        #I'm initializing it for time nn=N.\n",
    "\n",
    "        #You could make payoffDiscounted\n",
    "        #to be a matrix because it depends on nn.\n",
    "        #But I will just reuse a 1-dimensional array,\n",
    "        #by overwriting the time nn+1 entries at time nn.\n",
    "\n",
    "        for nn in np.arange(N-1,0,-1):\n",
    "            continuationPayoffDiscounted = np.exp(-r*dt)*payoffDiscounted\n",
    "            # This is the CONTINUATION payoff (cashflow) along each path,\n",
    "            # discounted to time nn (for nn=N-1,N-2,...)\n",
    "            # It corresponds to the blue column in each page of the Excel worksheet\n",
    "            # Note that payoffDiscounted comes from the previous iteration\n",
    "            # -- which was at time nn+1.  So now we discount back to time nn.\n",
    "\n",
    "            X=paths[:,nn-1]\n",
    "            exerciseValue = K-X\n",
    "\n",
    "            if self.algorithm == 'value':\n",
    "                # This is the value function (Longstaff-Schwartz) approach.  For problem 1a\n",
    "\n",
    "                basisfunctions = # FILL THIS IN.  You may use np.stack\n",
    "                        # This will be an M-by-3 array containing the basis functions (Same ones as L7.9-7.10, and Excel)\n",
    "\n",
    "                coefficients = # FILL THIS IN\n",
    "                        # This will be an array of 3 estimated \"betas\".\n",
    "\n",
    "                estimatedContinuationValue = # FILL THIS IN with an array of length M.\n",
    "                        # This is similar to the Red column in Excel\n",
    "\n",
    "                whichPathsToExercise = (exerciseValue >= np.maximum(estimatedContinuationValue,0))\n",
    "                        #This is a length-M array of Booleans\n",
    "\n",
    "            elif self.algorithm == 'policy':\n",
    "                # This is the policy optimization approach to Reinforcement learning.  For problem 1b\n",
    "\n",
    "                (a_opt,b_opt) = scipy.optimize.minimize(\n",
    "                    negofMCaverageOfExpectedPayouts,(0,0),args=(X,exerciseValue,continuationPayoffDiscounted),method='Nelder-Mead').x\n",
    "                    #Chose Nelder-Mead optimizer because it is generating reasonable results with minimal coding effort\n",
    "                    #But gradient methods, done properly, usually run faster\n",
    "\n",
    "                whichPathsToExercise =\n",
    "                    #FILL THIS IN, using the right-hand side of the last equation on the homework sheet\n",
    "                    #This obtains the hard exercise decision from the optimized soft exercise function\n",
    "                    #It should be a length-M array of Booleans (as it was in the \"value\" approach.\n",
    "                    #But here it comes from the softExercise function)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Unknown algorithm type')\n",
    "\n",
    "\n",
    "            payoffDiscounted[whichPathsToExercise] = # FILL THIS IN -- see the \"discounted cashflow along path\" column in Excel\n",
    "            payoffDiscounted[np.logical_not(whichPathsToExercise)] = # FILL THIS IN -- see the \"discounted cashflow along path\" column in Excel\n",
    "\n",
    "        # The time-0 calculation needs no regression\n",
    "        continuationPayoffDiscounted = np.exp(-r*dt)*payoffDiscounted;\n",
    "        estimatedContinuationValue = np.mean(continuationPayoffDiscounted);\n",
    "        putprice = max(K-S0,estimatedContinuationValue);\n",
    "\n",
    "        return(putprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgmUYic94GBS"
   },
   "outputs": [],
   "source": [
    "# for Policy optimization approach, problem 1b\n",
    "#\n",
    "# If b<<0 then this function essentially returns nearly 1 if X<a, or nearly 0 if X>a\n",
    "# but with some smoothing of the discontinuity, using a sigmoid function, to help the optimizer\n",
    "\n",
    "def softExercise(X,a,b):\n",
    "    return 1/(1+np.exp(-b*(X-a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fenSj43Z4GBS"
   },
   "outputs": [],
   "source": [
    "# for Policy optimization approach, problem 1b\n",
    "\n",
    "def negofMCaverageOfExpectedPayouts(coefficients, x, exercisePayoff, continuationPayoff):\n",
    "\n",
    "    p = softExercise(x,*coefficients)\n",
    "\n",
    "    # p and exercisePayoff and continuationPayoff are all length-M arrays\n",
    "\n",
    "    return # FILL THIS IN\n",
    "\n",
    "## You fill in, what to return.  It should be the negative of the expression inside the max() on the homework sheet.\n",
    "## Need to take the negative because we are calling \"minimize\" but we want to do _maximization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQwn3Yn44GBS"
   },
   "outputs": [],
   "source": [
    "hw7MC = MCengine(M=10000,N=4,seed=0,algorithm='value')\n",
    "hw7MC.price_americanPut_GBM(hw7contract,hw7dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNwP7DMR6hU4"
   },
   "outputs": [],
   "source": [
    "hw7MC = MCengine(M=10000,N=4,seed=0,algorithm='policy')\n",
    "hw7MC.price_americanPut_GBM(hw7contract,hw7dynamics)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
